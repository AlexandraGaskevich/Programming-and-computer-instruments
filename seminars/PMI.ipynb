{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Коллокации и Pointwise Mutual Information (PMI)\n",
    "\n",
    "**Коллокация** - сочетание из двух или более слов, которое:\n",
    "\n",
    "1. обладает некомпозициональной семантикой.\n",
    "2. состоит из двух слов, которые вместе значимо часто.\n",
    "\n",
    "При лингвистическом анализе текста коллокации нам очень нужны по понятным причинам:\n",
    "\n",
    "* выявить, о чем текст\n",
    "* найти различия между текстами на схожую тематику\n",
    "* находить упоминания каких-то событий и персон\n",
    "* и т.д.\n",
    "\n",
    "Значит, нам нужно уметь выделять их автоматически. Чтобы найти коллокации, состоящие из двух слов, нам нужно выделить такие биграммы, которые встречаются друг с другом значимо часто, по сравнению с их индивидуальными частотами.\n",
    "\n",
    "Для выделения коллокаций существуют разные метрики. Мы познакомимся с одной из них, которая называется **[Pointwise Mutual Information (PMI)](https://en.wikipedia.org/wiki/Pointwise_mutual_information)**. Формула рассчета PMI - \n",
    "\n",
    "$$ pmi = log \\frac{{p(x,y)}}{p(x) \\cdot p(y)} $$\n",
    "\n",
    "* x и y - слова, входящие в биграмму;\n",
    "* p(x,y) - вероятность появления биграммы x + y;\n",
    "* p(x) и p(y) -- вероятность появления каждого из элементов биграммы в отдельности.\n",
    "\n",
    "Значит, чтобы посчитать PMI, нам нужно:\n",
    "\n",
    "* взять текст\n",
    "* сделать частотный список всех слов в тексте\n",
    "* найти вероятность появления каждого слова в тексте, разделив его частотность на общее количество слов\n",
    "* сделать частотный список всех биграмм в тексте\n",
    "* найти вероятность появления каждой биграммы в тексте, разделив её частотность на общее количество биграмм (а сколько их?)\n",
    "* найти pmi по формуле\n",
    "* биграммы с наибольшим pmi - коллокации в этом тексте.\n",
    "\n",
    "Давайте попробуем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from math import log\n",
    "\n",
    "punct = '[.,!«»?&@\"$\\[\\]\\(\\):;%#&\\'—-]'\n",
    "\n",
    "def preprocessing(text):\n",
    "    text_wo_punct = re.sub(punct, '', text.lower())\n",
    "    words = text_wo_punct.strip().split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../news.txt', 'r', encoding='utf-8') as f:\n",
    "    words = preprocessing(f.read())\n",
    "\n",
    "word_freq = {}\n",
    "for word in words:\n",
    "    if word in word_freq:\n",
    "        word_freq[word] += 1\n",
    "    else:\n",
    "        word_freq[word] = 1\n",
    "\n",
    "bigrams = []\n",
    "for ind in range(1, len(words) - 1):\n",
    "    bigrams.append(' '.join([words[ind - 1], words[ind]]))\n",
    "    \n",
    "bigram_freq = {}\n",
    "for b in bigrams:\n",
    "    if b in bigram_freq:\n",
    "        bigram_freq[b] += 1\n",
    "    else:\n",
    "        bigram_freq[b] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_pmi(x, y):\n",
    "    p_xy = bigram_freq[' '.join([x, y])]\n",
    "    p_x, p_y = word_freq[x]/len(word_freq), word_freq[y]\n",
    "    pmi = log(p_xy/(p_x * p_y))\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сложную внешнеполитическую 10.229765142747272\n",
      "называемом одноканальном 10.229765142747272\n",
      "социальному служению 10.229765142747272\n",
      "часовых поясов 10.229765142747272\n",
      "de lex 10.229765142747272\n",
      "установлением конкретной 10.229765142747272\n",
      "судебным посыльным 10.229765142747272\n",
      "выбранные банкиагенты 10.229765142747272\n",
      "материальном вознаграждении 10.229765142747272\n",
      "татарстан тува 10.229765142747272\n",
      "обнесена инженерными 10.229765142747272\n",
      "замгубернатора восстановлен 10.229765142747272\n",
      "давнее знакомство 10.229765142747272\n",
      "шевкет кайбуллаев 10.229765142747272\n",
      "камуфляжная одежда 10.229765142747272\n",
      "носил непартийный 10.229765142747272\n",
      "страновыми сирия 10.229765142747272\n",
      "специ­фические вредные 10.229765142747272\n",
      "громкая либеральная 10.229765142747272\n",
      "конвертировала банковские 10.229765142747272\n",
      "сулейман керимов 10.229765142747272\n",
      "происходили оспариваемые 10.229765142747272\n",
      "курсовым разницам 10.229765142747272\n",
      "трудовому договору 10.229765142747272\n",
      "рента направлялись 10.229765142747272\n",
      "тебя накажем 10.229765142747272\n",
      "максимова 543 10.229765142747272\n",
      "приветствуем федерализацию 10.229765142747272\n",
      "со­основателей стивен 10.229765142747272\n",
      "очевидна массированное 10.229765142747272\n",
      "формирующегося российскокитайского 10.229765142747272\n",
      "изгоем считающаяся 10.229765142747272\n",
      "1473 единицы 10.229765142747272\n",
      "подвергнется аресту 10.229765142747272\n",
      "распада 300летнего 10.229765142747272\n",
      "аналитическую записку 10.229765142747272\n",
      "кондитеров сыр 10.229765142747272\n",
      "евангелических движений 10.229765142747272\n",
      "стены разработанный 10.229765142747272\n",
      "bahar energy 10.229765142747272\n",
      "сбыте наркотиков 10.229765142747272\n",
      "методика доступна 10.229765142747272\n",
      "красноречив оргвывод 10.229765142747272\n",
      "занимающая излишне 10.229765142747272\n",
      "выпендриться продуктовая 10.229765142747272\n",
      "нравственная патриотическая 10.229765142747272\n",
      "эротизма брежневских 10.229765142747272\n",
      "грамматическую ошибку 10.229765142747272\n",
      "обернулось пшиком 10.229765142747272\n",
      "osler hoskin 10.229765142747272\n",
      "гарантию бесперебойных 10.229765142747272\n",
      "баденвюртемберг landesbank 10.229765142747272\n",
      "марий эл 10.229765142747272\n",
      "контролируется боевиками 10.229765142747272\n",
      "ренкинге чешских 10.229765142747272\n",
      "вынужденно отошли 10.229765142747272\n",
      "ванкоре добыто 10.229765142747272\n",
      "european identity 10.229765142747272\n",
      "игоре изместьеве 10.229765142747272\n",
      "специализированного расчетного 10.229765142747272\n",
      "хургаду банкок 10.229765142747272\n",
      "обвиняла правителей 10.229765142747272\n",
      "извне осваивали 10.229765142747272\n",
      "очистке стоков 10.229765142747272\n",
      "паритет помножьте 10.229765142747272\n",
      "kapur surya 10.229765142747272\n",
      "горит обычное 10.229765142747272\n",
      "враждующих индию 10.229765142747272\n",
      "разгружают фургоны 10.229765142747272\n",
      "conocophillips райан 10.229765142747272\n",
      "тихий океан 10.229765142747272\n",
      "рифат шайхутдинов 10.229765142747272\n",
      "york mellon 10.229765142747272\n",
      "вопросах усиления 10.229765142747272\n",
      "порывы приобретают 10.229765142747272\n",
      "турецкое консульство 10.229765142747272\n",
      "султан батов 10.229765142747272\n",
      "электоральным феноменом 10.229765142747272\n",
      "кадровых обновлениях 10.229765142747272\n",
      "автоматами наперевес 10.229765142747272\n",
      "акционерном соглашении 10.229765142747272\n",
      "разовому значительному 10.229765142747272\n",
      "убедительную демократическую 10.229765142747272\n",
      "pbs coals 10.229765142747272\n",
      "внезапная перезагрузка 10.229765142747272\n",
      "невеликую склонность 10.229765142747272\n",
      "подвижных грунтовых 10.229765142747272\n",
      "массовыми случаями 10.229765142747272\n",
      "немецкий автопроизводитель 10.229765142747272\n",
      "sachs основавшие 10.229765142747272\n",
      "доверять выводам 10.229765142747272\n",
      "читай христианской 10.229765142747272\n",
      "обстрелу подвергся 10.229765142747272\n",
      "мс21 воплотятся 10.229765142747272\n",
      "придания правомерного 10.229765142747272\n",
      "саджан джиндалы 10.229765142747272\n",
      "решены туркменистан 10.229765142747272\n",
      "удалению бабая 10.229765142747272\n",
      "соединение наземных 10.229765142747272\n",
      "подготовленных диверсантов 10.229765142747272\n",
      "№37 академический 10.229765142747272\n"
     ]
    }
   ],
   "source": [
    "pmi = {}\n",
    "for bigr in bigrams:\n",
    "    x, y = bigr.split()\n",
    "    pmi[bigr] = count_pmi(x, y)\n",
    "\n",
    "i = 0\n",
    "for bigram in sorted(pmi, key = lambda m: -pmi[m]):\n",
    "    if i > 100:\n",
    "        break\n",
    "    print(bigram, pmi[bigram])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Какие слова имеют большой коэффициент pmi, а какие маленький?\n",
    "\n",
    "С помощью pmi можно также искать биграммы, специфичные для какой-то категории текстов. Для этого нам нужно всего лишь посчитать коэффициент PMI для слова/биграммы и категории.\n",
    "\n",
    "Например, у нас есть тексты 3 категорий. Посчитаем для слов в этих текстах коэффициент PMI, при этом в формуле X будет словом, а Y - категорией. (X, Y) - сколько раз слово X встретилось в текстах категории Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "anek = ''\n",
    "teh = ''\n",
    "izvest = ''\n",
    "for root, dirs, files in os.walk('texts'):\n",
    "    for f in files:\n",
    "        if 'anekdots' in root:\n",
    "            num_anek = len(files)\n",
    "            anek += open(os.path.join(root, f)).read()\n",
    "        elif 'izvest' in root:\n",
    "            num_izvest = len(files)\n",
    "            izvest += open(os.path.join(root, f)).read()\n",
    "        elif 'teh_mol' in root:\n",
    "            num_teh = len(files)\n",
    "            teh += open(os.path.join(root, f)).read()\n",
    "            \n",
    "words_anek = preprocessing(anek)\n",
    "words_teh = preprocessing(teh)\n",
    "words_izvest = preprocessing(izvest)\n",
    "\n",
    "words = words_anek + words_teh + words_izvest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def freq_dict(arr):\n",
    "    dic = {}\n",
    "    for element in arr:\n",
    "        if element in dic:\n",
    "            dic[element] += 1\n",
    "        else:\n",
    "            dic[element] = 1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_freq = freq_dict(words)\n",
    "anek_freq = freq_dict(words_anek)\n",
    "izvest_freq = freq_dict(words_izvest)\n",
    "teh_freq = freq_dict(words_teh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pmi_for_cats(x, y):\n",
    "    if y == 'anek':\n",
    "        dic = anek_freq\n",
    "        num = num_anek\n",
    "    elif y == 'teh':\n",
    "        dic = teh_freq\n",
    "        num = num_teh\n",
    "    elif y == 'izvest':\n",
    "        dic = izvest_freq\n",
    "        num = num_izvest\n",
    "    p_xy = dic[x]\n",
    "    p_x, p_y = corpus_freq[x], num\n",
    "    pmi = log(p_xy/(p_x * p_y))\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тяжелой teh\n",
      "дуэль anek\n",
      "станция teh\n",
      "концу teh\n",
      "мирового izvest\n",
      "целый anek\n",
      "назначению izvest\n",
      "народами anek\n",
      "собственными teh\n",
      "другой anek\n",
      "русских anek\n",
      "признание teh\n",
      "провели anek\n",
      "суд anek\n",
      "какаято anek\n",
      "планете teh\n",
      "грандиозная anek\n",
      "развитию izvest\n",
      "единицы teh\n",
      "инструменты anek\n",
      "бурно teh\n",
      "словом teh\n",
      "оставаться izvest\n",
      "удовлетворения anek\n",
      "прогресс teh\n",
      "подтверждение teh\n",
      "специальной teh\n",
      "200 anek\n",
      "цвета anek\n",
      "третьей anek\n",
      "оба anek\n",
      "справиться teh\n",
      "иностранного anek\n",
      "20 anek\n",
      "назвать anek\n",
      "удачно anek\n",
      "человек anek\n",
      "черных teh\n",
      "советским anek\n",
      "водах teh\n",
      "80х teh\n",
      "предложил anek\n",
      "слушал anek\n",
      "теле teh\n",
      "внутренних izvest\n",
      "чего anek\n",
      "участке anek\n",
      "покупка anek\n",
      "временем teh\n",
      "широкие teh\n",
      "окраине anek\n",
      "легкие anek\n",
      "эту anek\n",
      "приборов teh\n",
      "совпадает teh\n",
      "прошлой anek\n",
      "строгим anek\n",
      "получения teh\n",
      "большие anek\n",
      "дверь anek\n",
      "подобное teh\n",
      "открытие teh\n",
      "напоследок anek\n",
      "оказывают teh\n",
      "студенты teh\n",
      "мое anek\n",
      "41 anek\n",
      "великое teh\n",
      "90 teh\n",
      "лучшем izvest\n",
      "балки teh\n",
      "программ izvest\n",
      "обе anek\n",
      "конкретного teh\n",
      "род anek\n",
      "взяли anek\n",
      "разницы anek\n",
      "хранится teh\n",
      "доказала teh\n",
      "снега teh\n",
      "несмотря teh\n",
      "временами anek\n",
      "класса anek\n",
      "дно teh\n",
      "ветра teh\n",
      "откровенно anek\n",
      "заметно anek\n",
      "справедливо izvest\n",
      "телефону anek\n",
      "квалификации teh\n",
      "малое anek\n",
      "существующей izvest\n",
      "15 anek\n",
      "двух anek\n",
      "моря teh\n",
      "9 teh\n",
      "телефонных izvest\n",
      "студентам anek\n",
      "проведения teh\n",
      "реально izvest\n",
      "великий anek\n"
     ]
    }
   ],
   "source": [
    "cat_pmi = {}\n",
    "i = 0\n",
    "for word in corpus_freq:\n",
    "    if i > 100:\n",
    "        break\n",
    "    try:\n",
    "        pmi_anek = pmi_for_cats(word, 'anek')\n",
    "    except KeyError:\n",
    "        pmi_anek = 0\n",
    "    try:\n",
    "        pmi_teh = pmi_for_cats(word, 'teh')\n",
    "    except KeyError:\n",
    "        pmi_teh = 0\n",
    "    try:\n",
    "        pmi_izvest = pmi_for_cats(word, 'izvest')\n",
    "    except KeyError:\n",
    "        pmi_izvest = 0\n",
    "    max_pmi = max(pmi_anek, pmi_teh, pmi_izvest)\n",
    "    if max_pmi == 0:\n",
    "        continue\n",
    "    if max_pmi == pmi_anek:\n",
    "        cat = 'anek'\n",
    "    elif max_pmi == pmi_teh:\n",
    "        cat = 'teh'\n",
    "    elif max_pmi == pmi_izvest:\n",
    "        cat = 'izvest'\n",
    "    print(word, cat)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задания\n",
    "\n",
    "1. Отфильтровать слова, для которых мы вычисляем категории: убрать короткие и служебные, взять самые частотные.\n",
    "2. Соотнести биграммы с категориями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
